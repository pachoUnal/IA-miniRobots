# -*- coding: utf-8 -*-
"""RAGs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RU3ZIFlB_345PA6cqMVtHg3x-erTlEy8

#### 1. Instalación de librería
Se intala la librería para usar tranformers
"""

!pip install faiss-cpu transformers sentence-transformers

"""#### 2. Cargar corpus de documentos
Debido a restricciones por parte de las páginas oficiales de la Universidad Nacional de Colombia, se crea el siguiente corpus de documentos para ilustrar el RAG
"""

documentos = [
  "La presión por aumentar la estatura del caballo criollo colombiano ha llevado al uso excesivo de hormonas y vitaminas, poniendo en riesgo su bienestar animal.",  # :contentReference[oaicite:1]{index=1}
  "En el Atlántico, científicos detectaron 49 variantes genéticas en personas de 45 a 70 años que podrían anticipar riesgos de Alzheimer, Parkinson u otras demencias.",  # :contentReference[oaicite:2]{index=2}
  "Un estudio concluye que la represión policial hacia consumidores de bazuco es ineficaz y vulnera derechos humanos, recomendando un enfoque de salud pública.",  # :contentReference[oaicite:3]{index=3}
  "Investigadores en Antioquia desarrollaron un sistema con inteligencia artificial y satélites para predecir zonas críticas de deforestación hasta con dos años de antelación.",  # :contentReference[oaicite:4]{index=4}
  "Se implementaron 29 aulas STEAM en regiones apartadas, incluyendo San Basilio de Palenque, para fortalecer la educación en ciencia y tecnología.",  # :contentReference[oaicite:5]{index=5}
  "La alimentación de gallinas criollas con semillas de lino aumentó significativamente los niveles de omega‑3 en los huevos, mejorando su valor nutricional.",  # :contentReference[oaicite:6]{index=6}
]

"""#### 3. Vectorizar y construir el índice FAISS"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Cargar modelo de embeddings local
modelo_embeddings = SentenceTransformer('all-MiniLM-L6-v2')

# Vectorizar el corpus
vectores = modelo_embeddings.encode(documentos, show_progress_bar=True)

# Crear el índice FAISS
index = faiss.IndexFlatL2(vectores.shape[1])
index.add(np.array(vectores))

"""#### 4. Función para recuperar múltiples contextos"""

def buscar_contexto(pregunta, top_k=3):
    pregunta_vec = modelo_embeddings.encode([pregunta])
    _, indices = index.search(np.array(pregunta_vec), top_k)
    fragmentos = [documentos[i] for i in indices[0]]
    return " ".join(fragmentos)

"""#### 5. Cargar modelo generativo FLAN-T5 optimizado para QA"""

from transformers import pipeline

# Modelo multilingüe, sigue instrucciones y responde preguntas
generador = pipeline("text2text-generation", model="google/flan-t5-base")

"""#### 6. Función de respuesta mejorada con prompt claro"""

def responder(pregunta):
    contexto = buscar_contexto(pregunta, top_k=3)
    prompt = f"Contesta la siguiente pregunta usando solo la información del contexto.\n\nContexto: {contexto}\nPregunta: {pregunta}"
    resultado = generador(prompt, max_new_tokens=100)[0]['generated_text']
    return resultado.strip()

"""#### 7. Probar el sistema"""

print(responder("¿Cuántas aulas STEM hay?"))

print(responder("¿Puede alimentarse a la gallina con semillas de lino?"))

print(responder("¿Cuentame noticias de la Universidad Nacional de Colombia?"))